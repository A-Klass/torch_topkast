{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#%%\r\n",
    "import copy\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from sklearn import datasets\r\n",
    "from torch.utils.data import DataLoader, Dataset\r\n",
    "\r\n",
    "from topkast_linear import TopKastLinear\r\n",
    "from topkast_loss import TopKastLoss\r\n",
    "\r\n",
    "\r\n",
    "#%%\r\n",
    "class boston_dataset(Dataset):\r\n",
    "    \"\"\"Boston dataset.\"\"\"\r\n",
    "\r\n",
    "    def __init__(self):\r\n",
    "        self.dataset = datasets.load_boston()\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.dataset.data)\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        if torch.is_tensor(idx):\r\n",
    "            idx = idx.tolist()\r\n",
    "        \r\n",
    "        data = torch.from_numpy(self.dataset.data[idx])\r\n",
    "        target = torch.tensor(self.dataset.target[idx])\r\n",
    "        return data, target\r\n",
    "    \r\n",
    "#%%    \r\n",
    "def train(net, num_epochs, num_epochs_explore, update_every, loss,\r\n",
    "          batch_size, split=[.7, .2, .1], patience=5, lr=1e-3):\r\n",
    "\r\n",
    "    if len(split) < 3:\r\n",
    "        split.append(0)\r\n",
    "\r\n",
    "    train_count, validation_count, test_count = np.round(\r\n",
    "        np.multiply(boston_dataset().__len__(), split)).astype(int)\r\n",
    "    train_dataset, validation_dataset, test_dataset = \\\r\n",
    "    torch.utils.data.random_split(\r\n",
    "        boston_dataset(), (train_count, validation_count, test_count), \r\n",
    "        generator=torch.Generator().manual_seed(42))\r\n",
    "\r\n",
    "    train_dataset = DataLoader(\r\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\r\n",
    "\r\n",
    "    losses_validation = np.zeros(num_epochs)\r\n",
    "    losses_train = np.zeros(num_epochs)\r\n",
    "    best_loss = np.inf\r\n",
    "    best_epoch = 0\r\n",
    "    best_net = None\r\n",
    "\r\n",
    "    for epoch in range(num_epochs):\r\n",
    "        if epoch < num_epochs_explore:\r\n",
    "            for layer in net.children():\r\n",
    "                if isinstance(layer, TopKastLinear):\r\n",
    "                    layer.update_active_param_set()\r\n",
    "        else:\r\n",
    "            if epoch % update_every == 0:\r\n",
    "               for layer in net.children():\r\n",
    "                   if isinstance(layer, TopKastLinear):\r\n",
    "                        layer.update_active_param_set() \r\n",
    "        for X, y in train_dataset:\r\n",
    "            X = X.float()\r\n",
    "            y = y.float().reshape(-1, 1)\r\n",
    "            y_hat = net(X)\r\n",
    "            # optimizer.zero_grad()\r\n",
    "            loss_epoch = loss(y_hat, y)\r\n",
    "            loss_epoch.sum().backward(retain_graph = True)\r\n",
    "            # print(torch.linalg.norm(net.layer_in.weight_vector))\r\n",
    "            # optimizer.step()\r\n",
    "            sgd(net.parameters(), lr=lr, batch_size=batch_size)\r\n",
    "            # print(torch.linalg.norm(net.layer_in.weight_vector))\r\n",
    "            # print(torch.linalg.norm(net.layer_in.bias))\r\n",
    "            # print(torch.linalg.norm(net.layer_in.sparse_weights.grad.to_dense()))\r\n",
    "            losses_train[epoch] += loss_epoch / len(y)\r\n",
    "        with torch.no_grad(): \r\n",
    "            losses_validation[epoch] = loss(\r\n",
    "                net(validation_dataset[:][0].float(), sparse=False), \r\n",
    "                validation_dataset[:][1].float().reshape(-1, 1))\r\n",
    "        if (epoch + 1) % 100 == 0:\r\n",
    "            print(f'epoch {epoch + 1}, val loss {losses_validation[epoch]:f}, train loss {losses_train[epoch]:f}') \r\n",
    "        \r\n",
    "        # Compare this loss to the best current loss\r\n",
    "        # If it's better save the current net and change best loss\r\n",
    "        if losses_validation[epoch] < best_loss:\r\n",
    "            best_epoch = epoch\r\n",
    "            best_loss = losses_validation[epoch]\r\n",
    "            best_net = copy.deepcopy(net)\r\n",
    "\r\n",
    "        # Check if we are patience epochs away from the current best epoch, \r\n",
    "        # if that's the case break the training loop\r\n",
    "        if epoch - best_epoch > patience:\r\n",
    "            break\r\n",
    "    with torch.no_grad():\r\n",
    "        test_loss = loss(\r\n",
    "            net(test_dataset[:][0].float(), sparse=False), \r\n",
    "            test_dataset[:][1].float().reshape(-1, 1))\r\n",
    "\r\n",
    "    return best_net, losses_validation[1:(best_epoch + patience)], losses_train[1:(best_epoch + patience)], best_epoch, test_loss\r\n",
    "\r\n",
    "#%% Second Network\r\n",
    "class TopKastNet(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        self.layer_in = TopKastLinear(\r\n",
    "            13, 512, p_forward=0.6, p_backward=0.5)\r\n",
    "        self.activation = nn.ReLU()\r\n",
    "        self.hidden1 = TopKastLinear(\r\n",
    "            512, 512, p_forward=0.7, p_backward=0.5)\r\n",
    "        # self.hidden2 = TopKastLinear(\r\n",
    "        #     1024, 1024, p_forward=0.5, p_backward=0.4)\r\n",
    "        self.layer_out = TopKastLinear(\r\n",
    "            512, 1,\r\n",
    "            p_forward=0.6, p_backward=0.5)\r\n",
    "\r\n",
    "    def forward(self, X, sparse=True):\r\n",
    "        y = self.layer_in(X, sparse=sparse)\r\n",
    "        y = self.hidden1(self.activation(y), sparse=sparse)\r\n",
    "        # y = self.hidden2(self.activation(y), sparse=sparse)\r\n",
    "        \r\n",
    "        return self.layer_out(self.activation(y), sparse=sparse)\r\n",
    "\r\n",
    "#%%\r\n",
    "def sgd(params, lr, batch_size):\r\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\r\n",
    "    with torch.no_grad():\r\n",
    "        for param in params:\r\n",
    "            param -= lr * param.grad / batch_size\r\n",
    "            param.grad.zero_()\r\n",
    "            \r\n",
    "#%%\r\n",
    "net = TopKastNet()\r\n",
    "loss = TopKastLoss(loss=nn.MSELoss, net=net, alpha=0.7)\r\n",
    "# params = [child.sparse_weights for child in net.children() if isinstance(child, TopKastLinear)]\r\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=0.000001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kast_net, val_loss, train_loss, best_epoch, test_loss = train(\r\n",
    "    net=net, \r\n",
    "    num_epochs=30000, \r\n",
    "    num_epochs_explore=5000,\r\n",
    "    update_every=200,\r\n",
    "    loss=loss,\r\n",
    "    # optimizer=optimizer, \r\n",
    "    batch_size=128,\r\n",
    "    patience=100000,\r\n",
    "    lr=1e-3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(test_loss)\r\n",
    "# %%\r\n",
    "fig, axs = plt.subplots(2)\r\n",
    "axs[0].plot(range(len(train_loss)), train_loss)\r\n",
    "axs[0].set_title(\"training loss\")\r\n",
    "axs[1].plot(range(len(val_loss)), val_loss, color=\"red\")\r\n",
    "axs[1].set_title(\"validation loss\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('d2l': conda)"
  },
  "interpreter": {
   "hash": "da58119ef26f23e2d8474827559c39ae1ad354e1ce6ad588fd4f90d2accc97d7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}