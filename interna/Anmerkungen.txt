Anmerkungen

ctx.needs_input_grad: Vektor der gleichen L채nge wie Argumente in der forward-Methode (ausgenommen ctx)
                      Beschreibt ob der Input vom Gradienten betroffen ist.
ctx.save_for_backward: Sichert Tensors die vom Gradienten betroffen sind.
ctx.var = var: um vom Gradienten nicht betroffene Sachen f체r den Backward pass zu sparen (masken)

backward(): Innerhalb sollte, f체r alles was es braucht, der Gradient ausgerechnet werden
            Output muss die gleiche L채nge haben wie forward inputs (ohne ctx)

Garbage Notation:
* grad_output ist der Gradient des Losses in Relation zum Outputs des Layers (ohne activation function!!)
* input ist der input zu diesem layer 
* weight die weight Matrix

### Proof of Concept
MAYBE Resnet50 wie in dem Paper beschrieben. 