{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('d2l': conda)"
  },
  "interpreter": {
   "hash": "da58119ef26f23e2d8474827559c39ae1ad354e1ce6ad588fd4f90d2accc97d7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.sparse as sparse\n",
    "import numpy.random as random\n",
    "import numpy as np\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w)\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "true_w = torch.tensor([2, -3.4, 4.9, -1, 3.2, 0.35])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(indices[i:min(i +\n",
    "                                                   batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(features, W_hidden, w_out):\n",
    "    return torch.matmul(torch.matmul(features, W_hidden), w_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_sparse(features, W_hidden, w_out):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    hidden = sparse.mm(W_hidden, features.t())\n",
    "    return sparse.mm(w_out.t(), hidden)\n",
    "    # sparse.mm is very specific which matrices it gets. The first needs to sparse, the second needs to be strided/dense. So one needs to do a bunch of .t()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"Squared loss.\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape))**2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_kast_forward(w, kast = 0.5):\n",
    "    \"\"\"Selects 50% largest coefficients, not exactly like in paper, but for testing it should be fine.\"\"\"\n",
    "    # import pdb; pdb.set_trace()\n",
    "    threshold = np.quantile(w.detach().numpy().reshape(-1), 0.5)\n",
    "    mask = w < threshold\n",
    "    w[mask] = 0\n",
    "    return w.to_sparse().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask(w, kast = 0.5):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    threshold = np.quantile(w.detach().numpy().reshape(-1), 0.5)\n",
    "    mask = w < threshold\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  #@save\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            # import pdb; pdb.set_trace()\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1, loss 0.000059\n",
      "epoch 2, loss 0.000054\n",
      "epoch 3, loss 0.000056\n",
      "epoch 4, loss 0.000053\n",
      "epoch 5, loss 0.000055\n",
      "epoch 6, loss 0.000054\n",
      "epoch 7, loss 0.000058\n",
      "epoch 8, loss 0.000060\n",
      "epoch 9, loss 0.000055\n",
      "epoch 10, loss 0.000054\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 10\n",
    "net = forward_sparse\n",
    "loss = squared_loss\n",
    "batch_size = 10\n",
    "\n",
    "W_hidden = torch.normal(0, 0.01, size=(6, 6), requires_grad = True).to_sparse()\n",
    "w_out = torch.normal(0, 0.01, size=(6, 1), requires_grad = True).to_sparse()\n",
    "W_hidden.retain_grad()\n",
    "w_out.retain_grad()\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        # Make sparse forward pass\n",
    "        # mask_hidden = compute_mask(W_hidden)\n",
    "        # mask_out = compute_mask(w_out)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        y_hat = net(X, W_hidden, w_out)\n",
    "        l = loss(y_hat, y)  # Minibatch loss in `X` and `y`\n",
    "        l.sum().backward()\n",
    "        sgd([W_hidden, w_out], lr, batch_size)  # Update parameters using their gradient\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features, W_hidden, w_out), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}