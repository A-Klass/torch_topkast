{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('d2l': conda)"
  },
  "interpreter": {
   "hash": "da58119ef26f23e2d8474827559c39ae1ad354e1ce6ad588fd4f90d2accc97d7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.sparse as sparse\n",
    "import numpy.random as random\n",
    "import numpy as np\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w)\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "true_w = torch.tensor([2, -3.4, 4.9, -1, 3.2, 0.35])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(indices[i:min(i +\n",
    "                                                   batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(features, W_hidden, w_out):\n",
    "    return torch.matmul(torch.matmul(features, W_hidden), w_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_sparse(features, W_hidden, w_out):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    hidden = sparse.mm(W_hidden, features.t())\n",
    "    return sparse.mm(w_out.t(), hidden)\n",
    "    # sparse.mm is very specific which matrices it gets. The first needs to sparse, the second needs to be strided/dense. So one needs to do a bunch of .t()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"Squared loss.\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape))**2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_kast_forward(w, kast = 0.5):\n",
    "    \"\"\"Selects 50% largest coefficients, not exactly like in paper, but for testing it should be fine.\"\"\"\n",
    "    # import pdb; pdb.set_trace()\n",
    "    threshold = np.quantile(w.detach().numpy().reshape(-1), 0.5)\n",
    "    mask = w < threshold\n",
    "    w[mask] = 0\n",
    "    return w.to_sparse().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask(w, kast = 0.5):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    threshold = np.quantile(w.detach().numpy().reshape(-1), 0.5)\n",
    "    mask = w < threshold\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  #@save\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            import pdb; pdb.set_trace()\n",
    "            param = - lr * param.grad_fn(param) / batch_size + param\n",
    "            # param.grad_fn(param).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> <ipython-input-81-1d4ccb0012e2>(6)sgd()\n",
      "-> param = - lr * param.grad_fn(param) / batch_size + param\n",
      "tensor([[-0.0142, -0.0018, -0.0078,  0.0069,  0.0094, -0.0056],\n",
      "        [ 0.0133,  0.0110, -0.0096, -0.0041,  0.0116,  0.0175],\n",
      "        [ 0.0051,  0.0221, -0.0058, -0.0092,  0.0137,  0.0049],\n",
      "        [-0.0003, -0.0025,  0.0013,  0.0089,  0.0004, -0.0026],\n",
      "        [-0.0158,  0.0024, -0.0023,  0.0232, -0.0005,  0.0012],\n",
      "        [-0.0012,  0.0208, -0.0080,  0.0032, -0.0088,  0.0045]])\n",
      "tensor(indices=tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3,\n",
      "                        3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5],\n",
      "                       [0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0,\n",
      "                        1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5]]),\n",
      "       values=tensor([-0.0142, -0.0018, -0.0078,  0.0069,  0.0094, -0.0056,\n",
      "                       0.0133,  0.0110, -0.0096, -0.0041,  0.0116,  0.0175,\n",
      "                       0.0051,  0.0221, -0.0058, -0.0092,  0.0137,  0.0049,\n",
      "                      -0.0003, -0.0025,  0.0013,  0.0089,  0.0004, -0.0026,\n",
      "                      -0.0158,  0.0024, -0.0023,  0.0232, -0.0005,  0.0012,\n",
      "                      -0.0012,  0.0208, -0.0080,  0.0032, -0.0088,  0.0045]),\n",
      "       size=(6, 6), nnz=36, layout=torch.sparse_coo)\n",
      "tensor([[-0.0142, -0.0018, -0.0078,  0.0069,  0.0094, -0.0056],\n",
      "        [ 0.0133,  0.0110, -0.0096, -0.0041,  0.0116,  0.0175],\n",
      "        [ 0.0051,  0.0221, -0.0058, -0.0092,  0.0137,  0.0049],\n",
      "        [-0.0003, -0.0025,  0.0013,  0.0089,  0.0004, -0.0026],\n",
      "        [-0.0158,  0.0024, -0.0023,  0.0232, -0.0005,  0.0012],\n",
      "        [-0.0012,  0.0208, -0.0080,  0.0032, -0.0088,  0.0045]])\n",
      "tensor(indices=tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3,\n",
      "                        3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5],\n",
      "                       [0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0,\n",
      "                        1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5]]),\n",
      "       values=tensor([-0.0142, -0.0018, -0.0078,  0.0069,  0.0094, -0.0056,\n",
      "                       0.0133,  0.0110, -0.0096, -0.0041,  0.0116,  0.0175,\n",
      "                       0.0051,  0.0221, -0.0058, -0.0092,  0.0137,  0.0049,\n",
      "                      -0.0003, -0.0025,  0.0013,  0.0089,  0.0004, -0.0026,\n",
      "                      -0.0158,  0.0024, -0.0023,  0.0232, -0.0005,  0.0012,\n",
      "                      -0.0012,  0.0208, -0.0080,  0.0032, -0.0088,  0.0045]),\n",
      "       size=(6, 6), nnz=36, layout=torch.sparse_coo, grad_fn=<ToSparseBackward>)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "BdbQuit",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-6befc5ac63b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Minibatch loss in `X` and `y`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mW_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_out\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Update parameters using their gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mtrain_l\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-1d4ccb0012e2>\u001b[0m in \u001b[0;36msgd\u001b[1;34m(params, lr, batch_size)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mparam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[1;31m# param.grad_fn(param).zero_()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-1d4ccb0012e2>\u001b[0m in \u001b[0;36msgd\u001b[1;34m(params, lr, batch_size)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mparam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[1;31m# param.grad_fn(param).zero_()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;31m# None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'line'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'call'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 0.3\n",
    "num_epochs = 5\n",
    "net = forward_sparse\n",
    "loss = squared_loss\n",
    "batch_size = 10\n",
    "\n",
    "W_hidden = torch.normal(0, 0.01, size=(6, 6), requires_grad = True).to_sparse()\n",
    "w_out = torch.normal(0, 0.01, size=(6, 1), requires_grad = True).to_sparse()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        # Make sparse forward pass\n",
    "        # mask_hidden = compute_mask(W_hidden)\n",
    "        # mask_out = compute_mask(w_out)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        y_hat = net(X, W_hidden, w_out)\n",
    "        l = loss(y_hat, y)  # Minibatch loss in `X` and `y`\n",
    "        l.sum().backward()\n",
    "        sgd([W_hidden, w_out], lr, batch_size)  # Update parameters using their gradient\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features, W_hidden, w_out), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hidden = torch.normal(0, 0.01, size=(6, 6), requires_grad = True).to_sparse()\n",
    "w_out = torch.normal(0, 0.01, size=(6, 1), requires_grad = True).to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 2.3957e-04, -3.5383e-05, -1.7071e-05,  7.1259e-05,  3.4187e-04,\n",
       "          2.3327e-05, -4.3133e-04,  4.7800e-05,  3.6476e-05,  5.9954e-04]],\n",
       "       grad_fn=<SparseAddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "y_hat = forward_sparse(features[:10,:], W_hidden, w_out)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.2055,  0.5528, 32.2029, 74.8982, 19.1788,  6.0041,  2.2378, 39.9689,\n",
       "         49.9687, 51.1675]], grad_fn=<DivBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "l = squared_loss(y_hat, labels[:10])\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-1.9062e-03,  1.2920e-03, -1.1964e-02, -2.4589e-02, -5.5270e-03,\n",
       "         -4.1436e-03],\n",
       "        [-4.9148e-03,  1.6498e-03, -2.9532e-03, -8.2215e-03,  9.5527e-03,\n",
       "          4.5420e-03],\n",
       "        [ 1.5493e-03, -3.6528e-03, -1.1500e-02,  1.7868e-02,  7.0601e-03,\n",
       "          7.4590e-03],\n",
       "        [-1.1612e-02, -4.5305e-03,  4.8653e-03,  3.1775e-03, -3.3420e-04,\n",
       "          2.4657e-03],\n",
       "        [-3.9694e-03,  1.8187e-02, -3.5265e-05,  2.2798e-02,  1.3404e-02,\n",
       "          8.6537e-03],\n",
       "        [ 5.7849e-03,  2.3365e-03,  1.0150e-02, -1.8158e-02,  8.0625e-03,\n",
       "         -6.3099e-03]], grad_fn=<ToDenseBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "w_out.grad_fn(W_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-41-6dc0f0d2a335>:6: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n  if param.grad is not None:\n"
     ]
    }
   ],
   "source": [
    "sgd([W_hidden, w_out], lr, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}