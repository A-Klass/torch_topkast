{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('d2l': conda)"
  },
  "interpreter": {
   "hash": "da58119ef26f23e2d8474827559c39ae1ad354e1ce6ad588fd4f90d2accc97d7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.sparse as sparse\n",
    "import numpy.random as random\n",
    "import numpy as np\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w)\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "true_w = torch.tensor([2, -3.4, 4.9, -1, 3.2, 0.35])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(indices[i:min(i +\n",
    "                                                   batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(features, W_hidden, w_out):\n",
    "    return torch.matmul(torch.matmul(features, W_hidden), w_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_sparse(features, W_hidden, w_out):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    hidden = sparse.mm(W_hidden, features.t())\n",
    "    return sparse.mm(w_out.t(), hidden)\n",
    "    # sparse.mm is very specific which matrices it gets. The first needs to sparse, the second needs to be strided/dense. So one needs to do a bunch of .t()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"Squared loss.\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape))**2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_kast_forward(w, kast = 0.5):\n",
    "    \"\"\"Selects 50% largest coefficients, not exactly like in paper, but for testing it should be fine.\"\"\"\n",
    "    # import pdb; pdb.set_trace()\n",
    "    threshold = np.quantile(w.detach().numpy().reshape(-1), 0.5)\n",
    "    mask = w < threshold\n",
    "    w[mask] = 0\n",
    "    return w.to_sparse().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask(w, kast = 0.5):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    if w.is_sparse:\n",
    "        threshold = np.quantile(w.values().detach(), 0.5)\n",
    "        mask = w.values().detach() > threshold\n",
    "    else:\n",
    "        threshold = np.quantile(w.reshape(-1).detach(), 0.5)\n",
    "        mask = w.reshape(-1).detach() > threshold\n",
    "    return mask.reshape(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  #@save\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            # import pdb; pdb.set_trace()\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 0.003\n",
    "num_epochs = 10\n",
    "net = forward_sparse\n",
    "loss = squared_loss\n",
    "batch_size = 10\n",
    "\n",
    "W_hidden = torch.normal(0, 0.01, size=(6, 6), requires_grad = False)\n",
    "w_out = torch.normal(0, 0.01, size=(6, 1), requires_grad = False)\n",
    "# W_hidden.retain_grad() # .to_sparse() makes parameters not leave parameters anymore, so to train those parameters you need to retain the grad.\n",
    "# w_out.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-15-1b4d2e82a4e8>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  W_hidden_forward = torch.tensor(W_hidden * mask_hidden_forward, requires_grad = True).to_sparse()\n",
      "<ipython-input-15-1b4d2e82a4e8>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_out_forward = torch.tensor(w_out * mask_out_forward, requires_grad = True).to_sparse()\n",
      "epoch 1, loss 25.792978\n",
      "epoch 2, loss 24.640064\n",
      "epoch 3, loss 9.558971\n",
      "epoch 4, loss 5.868670\n",
      "epoch 5, loss 5.872594\n",
      "epoch 6, loss 5.860676\n",
      "epoch 7, loss 5.863440\n",
      "epoch 8, loss 5.857641\n",
      "epoch 9, loss 5.854462\n",
      "epoch 10, loss 5.836637\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        # Make sparse forward pass\n",
    "        mask_hidden_forward = compute_mask(W_hidden)\n",
    "        mask_out_forward = compute_mask(w_out)\n",
    "\n",
    "        W_hidden_forward = torch.tensor(W_hidden * mask_hidden_forward, requires_grad = True).to_sparse()\n",
    "        W_hidden_forward.retain_grad()\n",
    "        w_out_forward = torch.tensor(w_out * mask_out_forward, requires_grad = True).to_sparse()\n",
    "        w_out_forward.retain_grad()\n",
    "\n",
    "        # mask_hidden_backward = compute_mask(W_hidden, kast=0.7).to_sparse()\n",
    "        # mask_out_backward = compute_mask(w_out, kast=0.7).to_sparse()\n",
    "        # W_hidden_backward = (W_hidden * mask_hidden_backward).retain_grad()\n",
    "        # w_out_backward = (w_out * mask_out_backward).retain_grad()\n",
    "        # import pdb; pdb.set_trace()\n",
    "        y_hat = net(X, W_hidden_forward, w_out_forward)\n",
    "        l = loss(y_hat, y)  # Minibatch loss in `X` and `y`\n",
    "        l.sum().backward()\n",
    "        # import pdb; pdb.set_trace()\n",
    "        sgd([W_hidden_forward, w_out_forward], lr, batch_size)  # Update parameters using their gradient\n",
    "\n",
    "        # Inverse the masks. So that you can change the values in dense Parameter Matrix\n",
    "        mask_hidden_forward = mask_hidden_forward == False\n",
    "        mask_out_forward = mask_out_forward == False\n",
    "\n",
    "        W_hidden = (W_hidden * mask_hidden_forward + W_hidden_forward).detach()\n",
    "        w_out = (w_out * mask_out_forward + w_out_forward).detach()\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features, W_hidden, w_out), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}