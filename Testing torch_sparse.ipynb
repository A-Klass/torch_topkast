{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('d2l': conda)"
  },
  "interpreter": {
   "hash": "da58119ef26f23e2d8474827559c39ae1ad354e1ce6ad588fd4f90d2accc97d7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.sparse as sparse\n",
    "import numpy.random as random\n",
    "import numpy as np\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "from torch import autograd\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w)\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "true_w = torch.tensor([2, -3.4, 4.9, -1, 3.2, 0.35])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(indices[i:min(i +\n",
    "                                                   batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(features, W_hidden, w_out):\n",
    "    return torch.matmul(torch.matmul(features, W_hidden), w_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_sparse(features, W_hidden, w_out):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    hidden = sparse.mm(W_hidden, features.t())\n",
    "    return sparse.mm(w_out.t(), hidden)\n",
    "    # sparse.mm is very specific which matrices it gets. The first needs to sparse, the second needs to be strided/dense. So one needs to do a bunch of .t()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"Squared loss.\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape))**2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_kast_forward(w, kast = 0.5):\n",
    "    \"\"\"Selects 50% largest coefficients, not exactly like in paper, but for testing it should be fine.\"\"\"\n",
    "    # import pdb; pdb.set_trace()\n",
    "    threshold = np.quantile(w.detach().numpy().reshape(-1), 0.5)\n",
    "    mask = w < threshold\n",
    "    w[mask] = 0\n",
    "    return w.to_sparse().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask(w, kast = 0.5):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    if w.is_sparse:\n",
    "        threshold = np.quantile(w.values().detach(), kast)\n",
    "        mask = w.values().detach() > threshold\n",
    "    else:\n",
    "        threshold = np.quantile(w.reshape(-1).detach(), kast)\n",
    "        mask = w.reshape(-1).detach() > threshold\n",
    "    return mask.reshape(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  #@save\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            # import pdb; pdb.set_trace()\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 0.003\n",
    "num_epochs = 10\n",
    "net = forward_sparse\n",
    "loss = squared_loss\n",
    "batch_size = 10\n",
    "\n",
    "W_hidden = torch.normal(0, 0.01, size=(6, 6), requires_grad = False)\n",
    "w_out = torch.normal(0, 0.01, size=(6, 1), requires_grad = False)\n",
    "# W_hidden.retain_grad() # .to_sparse() makes parameters not leave parameters anymore, so to train those parameters you need to retain the grad.\n",
    "# w_out.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-20-3d7f25cdb87b>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  W_hidden_forward = torch.tensor(W_hidden * mask_hidden_forward, requires_grad = True).to_sparse()\n",
      "<ipython-input-20-3d7f25cdb87b>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_out_forward = torch.tensor(w_out * mask_out_forward, requires_grad = True).to_sparse()\n",
      "> <ipython-input-20-3d7f25cdb87b>(21)<module>()\n",
      "-> sgd([W_hidden_forward, w_out_forward], lr, batch_size)  # Update parameters using their gradient\n",
      "tensor(indices=tensor([[0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5],\n",
      "                       [0, 1, 4, 1, 2, 0, 2, 4, 5, 1, 5, 0, 1, 4, 0, 2, 4, 5]]),\n",
      "       values=tensor([ 1.0873e-02,  5.3343e-02, -2.9064e-03,  5.0835e-02,\n",
      "                      -9.5721e-04,  7.0614e-01,  1.8443e+00,  1.2701e+00,\n",
      "                       1.3572e-01,  4.8863e-02,  1.3810e-02,  4.3484e-03,\n",
      "                       5.3805e-02,  3.5719e-03,  2.8024e-01,  7.5755e-01,\n",
      "                       3.4717e-01,  5.7720e-02]),\n",
      "       size=(6, 6), nnz=18, layout=torch.sparse_coo, grad_fn=<ToSparseBackward>)\n",
      "tensor(indices=tensor([[0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5],\n",
      "                       [0, 1, 4, 1, 2, 0, 2, 4, 5, 1, 5, 0, 1, 4, 0, 2, 4, 5]]),\n",
      "       values=tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "                       37.0461,  26.7334,  -6.6708, -25.3859,  -0.9526,\n",
      "                        0.5588,   0.0000,   0.0000,   0.0000,  13.8679,\n",
      "                       10.0074,  -2.4971,  -9.5030]),\n",
      "       size=(6, 6), nnz=18, layout=torch.sparse_coo)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "BdbQuit",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-3d7f25cdb87b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mW_hidden_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_out_forward\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Update parameters using their gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# Inverse the masks. So that you can change the values in dense Parameter Matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-3d7f25cdb87b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mW_hidden_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_out_forward\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Update parameters using their gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# Inverse the masks. So that you can change the values in dense Parameter Matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;31m# None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'line'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'call'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        # Make sparse forward pass\n",
    "        mask_hidden_forward = compute_mask(W_hidden)\n",
    "        mask_out_forward = compute_mask(w_out)\n",
    "\n",
    "        W_hidden_forward = torch.tensor(W_hidden * mask_hidden_forward, requires_grad = True).to_sparse()\n",
    "        W_hidden_forward.retain_grad()\n",
    "        w_out_forward = torch.tensor(w_out * mask_out_forward, requires_grad = True).to_sparse()\n",
    "        w_out_forward.retain_grad()\n",
    "\n",
    "        # mask_hidden_backward = compute_mask(W_hidden, kast=0.7).to_sparse()\n",
    "        # mask_out_backward = compute_mask(w_out, kast=0.7).to_sparse()\n",
    "        # W_hidden_backward = (W_hidden * mask_hidden_backward).retain_grad()\n",
    "        # w_out_backward = (w_out * mask_out_backward).retain_grad()\n",
    "        # import pdb; pdb.set_trace()\n",
    "        y_hat = net(X, W_hidden_forward, w_out_forward)\n",
    "        l = loss(y_hat, y)  # Minibatch loss in `X` and `y`\n",
    "        l.sum().backward()\n",
    "        # import pdb; pdb.set_trace()\n",
    "        sgd([W_hidden_forward, w_out_forward], lr, batch_size)  # Update parameters using their gradient\n",
    "\n",
    "        # Inverse the masks. So that you can change the values in dense Parameter Matrix\n",
    "        mask_hidden_forward = mask_hidden_forward == False\n",
    "        mask_out_forward = mask_out_forward == False\n",
    "\n",
    "        W_hidden = (W_hidden * mask_hidden_forward + W_hidden_forward).detach()\n",
    "        w_out = (w_out * mask_out_forward + w_out_forward).detach()\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features, W_hidden, w_out), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer(nn.Module):\n",
    "\n",
    "    dense = parameter\n",
    "    mask = mask\n",
    "\n",
    "    def masking():\n",
    "    \n",
    "\n",
    "    def update_dense():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on function grad in module torch.autograd:\n\ngrad(outputs: Union[torch.Tensor, Sequence[torch.Tensor]], inputs: Union[torch.Tensor, Sequence[torch.Tensor]], grad_outputs: Union[torch.Tensor, Sequence[torch.Tensor], NoneType] = None, retain_graph: Union[bool, NoneType] = None, create_graph: bool = False, only_inputs: bool = True, allow_unused: bool = False) -> Tuple[torch.Tensor, ...]\n    Computes and returns the sum of gradients of outputs w.r.t. the inputs.\n    \n    ``grad_outputs`` should be a sequence of length matching ``output``\n    containing the \"vector\" in Jacobian-vector product, usually the pre-computed\n    gradients w.r.t. each of the outputs. If an output doesn't require_grad,\n    then the gradient can be ``None``).\n    \n    If ``only_inputs`` is ``True``, the function will only return a list of gradients\n    w.r.t the specified inputs. If it's ``False``, then gradient w.r.t. all remaining\n    leaves will still be computed, and will be accumulated into their ``.grad``\n    attribute.\n    \n    .. note::\n    \n        If you run any forward ops, create ``grad_outputs``, and/or call ``grad``\n        in a user-specified CUDA stream context, see\n        :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\n    \n    Args:\n        outputs (sequence of Tensor): outputs of the differentiated function.\n        inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be\n            returned (and not accumulated into ``.grad``).\n        grad_outputs (sequence of Tensor): The \"vector\" in the Jacobian-vector product.\n            Usually gradients w.r.t. each output. None values can be specified for scalar\n            Tensors or ones that don't require grad. If a None value would be acceptable\n            for all grad_tensors, then this argument is optional. Default: None.\n        retain_graph (bool, optional): If ``False``, the graph used to compute the grad\n            will be freed. Note that in nearly all cases setting this option to ``True``\n            is not needed and often can be worked around in a much more efficient\n            way. Defaults to the value of ``create_graph``.\n        create_graph (bool, optional): If ``True``, graph of the derivative will\n            be constructed, allowing to compute higher order derivative products.\n            Default: ``False``.\n        allow_unused (bool, optional): If ``False``, specifying inputs that were not\n            used when computing outputs (and therefore their grad is always zero)\n            is an error. Defaults to ``False``.\n\n"
     ]
    }
   ],
   "source": [
    "help(autograd.grad)"
   ]
  }
 ]
}