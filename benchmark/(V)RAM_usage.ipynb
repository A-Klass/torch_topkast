{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "91040858754dd64a0b3fa7c3625dc8dfca1149f67bbd2cbf8d9a8e1fad36938a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This notebook shows the differences of sparse vs. dense matrix operations and illustrates why we opted for pytorch_sparse.\n",
    "The key operation here is the outer product of a masked (sparse) matrix with a dense vector. We make a big weight matrix to illustrate that the outer product of a sparse matrix with a dense vector works best with torch_sparse.spmm(). Before every operation, the memory is cleaned up and the temporary variable 'temo' is reassigned, just to be sure."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install torch-scatter torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.1+cu111.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch_sparse\n",
    "from torch_sparse import spmm\n",
    "import gc # garbage collector to \"flush\" the memory"
   ]
  },
  {
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise SystemExit(\"you need a GPU with CUDA to run this notebook!\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask(w, topk_percentage):\n",
    "    \"\"\"\n",
    "    get the indices of the parameters to keep.\n",
    "    \"\"\"\n",
    "    threshold = np.quantile(w.reshape(-1), topk_percentage)\n",
    "    return np.where(w >= threshold)"
   ]
  },
  {
   "source": [
    "In case the graphics card has not been used at all since powering up, we set up a tiny variable and move it to the GPU in order to be able to call torch.cuda.memory_snapshot()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'device': 0,\n",
       "  'address': 21533556736,\n",
       "  'total_size': 2097152,\n",
       "  'allocated_size': 512,\n",
       "  'active_size': 512,\n",
       "  'segment_type': 'small',\n",
       "  'blocks': [{'size': 512, 'state': 'active_allocated'},\n",
       "   {'size': 2096640, 'state': 'inactive'}]}]"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "if len(torch.cuda.memory_snapshot()) == 0:\n",
    "    torch.tensor(8).cuda()\n",
    "assert len(torch.cuda.memory_snapshot()) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_memory=torch.cuda.memory_snapshot()[0]['total_size']\n",
    "n = int(available_memory / 4) # dimension of square matrix\n",
    "sparsity_1=0.99\n",
    "# sparsity_2=0.7\n",
    "nnz = int((1-sparsity_1)*n) # number of non-sparsified values\n",
    "rows = np.random.randint(0, n, nnz)\n",
    "cols = np.random.randint(0, n, nnz)\n",
    "values = torch.randn(nnz)\n",
    "X_sparse = torch.sparse_coo_tensor([rows,cols], values, size=(n,n)).cuda().requires_grad_(True)\n",
    "Y_dense = torch.randn((n,200)).cuda().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU memory allocated: 0.419535872\n",
      "GPU memory cached: 0.421527552\n",
      "Difference in allocated GPU memory: 0.4194304\n",
      "Difference in cached GPU memory: 0.8598323199999999\n",
      "101 ms ± 270 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "temp = torch.Tensor(8)\n",
    "initial_allocated_memory = torch.cuda.memory_allocated(device=None)/10**9\n",
    "init_cached_memory = torch.cuda.memory_reserved(device=None)/10**9\n",
    "print(f'GPU memory allocated: {torch.cuda.memory_allocated(device=None)/10**9}')\n",
    "print(f'GPU memory cached: {torch.cuda.memory_reserved(device=None)/10**9}')\n",
    "temp = torch.sparse.mm(X_sparse, Y_dense)\n",
    "print(f'Difference in allocated GPU memory: {torch.cuda.memory_allocated(device=None)/10**9 - initial_allocated_memory}')\n",
    "print(f'Difference in cached GPU memory: {torch.cuda.memory_reserved(device=None)/10**9 - init_cached_memory}')\n",
    "\n",
    "%timeit torch.sparse.mm(X_sparse, Y_dense)\n",
    "\n",
    "del temp\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU memory allocated: 0.419535872\n",
      "GPU memory cached: 0.421527552\n",
      "Difference in allocated GPU memory: 0.41953536\n",
      "Difference in cached GPU memory: 0.44040192\n",
      "4.39 ms ± 48.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "temp = torch.Tensor(8)\n",
    "initial_allocated_memory = torch.cuda.memory_allocated(device=None)/10**9\n",
    "init_cached_memory = torch.cuda.memory_reserved(device=None)/10**9\n",
    "print(f'GPU memory allocated: {torch.cuda.memory_allocated(device=None)/10**9}')\n",
    "print(f'GPU memory cached: {torch.cuda.memory_reserved(device=None)/10**9}')\n",
    "temp = spmm(torch.tensor([rows, cols], dtype=torch.int64).cuda(), values.cuda(), n, n, Y_dense)\n",
    "print(f'Difference in allocated GPU memory: {torch.cuda.memory_allocated(device=None)/10**9 - initial_allocated_memory}')\n",
    "print(f'Difference in cached GPU memory: {torch.cuda.memory_reserved(device=None)/10**9 - init_cached_memory}')\n",
    "\n",
    "%timeit spmm(torch.tensor([rows, cols], dtype=torch.int64).cuda(), values.cuda(), n, n, Y_dense)\n",
    "\n",
    "del temp\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU memory allocated: 0.419535872\nGPU memory cached: 0.421527552\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 GiB (GPU 0; 6.00 GiB total capacity; 400.10 MiB already allocated; 4.29 GiB free; 402.00 MiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-d9f4674069ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'GPU memory allocated: {torch.cuda.memory_allocated(device=None)/10**9}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'GPU memory cached: {torch.cuda.memory_reserved(device=None)/10**9}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_sparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_dense\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Difference in allocated GPU memory: {torch.cuda.memory_allocated(device=None)/10**9 - initial_allocated_memory}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Difference in cached GPU memory: {torch.cuda.memory_reserved(device=None)/10**9 - init_cached_memory}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 GiB (GPU 0; 6.00 GiB total capacity; 400.10 MiB already allocated; 4.29 GiB free; 402.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "temp = torch.Tensor(8)\n",
    "initial_allocated_memory = torch.cuda.memory_allocated(device=None)/10**9\n",
    "init_cached_memory = torch.cuda.memory_reserved(device=None)/10**9\n",
    "print(f'GPU memory allocated: {torch.cuda.memory_allocated(device=None)/10**9}')\n",
    "print(f'GPU memory cached: {torch.cuda.memory_reserved(device=None)/10**9}')\n",
    "temp = torch.matmul(X_sparse.to_dense().cuda(), Y_dense)\n",
    "print(f'Difference in allocated GPU memory: {torch.cuda.memory_allocated(device=None)/10**9 - initial_allocated_memory}')\n",
    "print(f'Difference in cached GPU memory: {torch.cuda.memory_reserved(device=None)/10**9 - init_cached_memory}')\n",
    "\n",
    "%timeit torch.matmul(X_sparse.to_dense().cuda(), Y_dense)\n",
    "\n",
    "del temp\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "source": [
    "Conclusion: torch.sparse.mm() is slower than torch_sparse.spmm() and requires more memory. torch.matmul() cannot even carry out the required operation, running out of memory."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}